{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a93bd80-c08d-4242-810c-f809588258f5",
   "metadata": {},
   "source": [
    "# UCLA, 170N Final Project\n",
    "John Parrack, Sam Eisenbach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d73eb3-86c2-4131-aa6c-dde0128f0ebd",
   "metadata": {},
   "source": [
    "### Solving Schrödinger's Equation using Finite Difference and Physics Informed Neural Network Algorithms\n",
    "\n",
    "In the realm of computational physics, solving partial differential equations (PDEs) stands as a cornerstone for understanding complex physical phenomena.This project is designed to delve into this intricate domain by employing two distinct methods: the Finite Difference Method (FDM) and  the Physics-Informed Neural Networs (PINN), both applied to a classic PDE problem. We have chosen the one-dimensional Schrödinger's Equation with a infinite square well potential as our test case.\n",
    "\n",
    "This equation, fundamental in quantum mechanics, describes how the quantum state of a physical system changes over time. In our case, the simplicity of the infinite square well potential enables us to access an analytical solution, providing a solid ground for comparison and evaluation of our computational approaches.\n",
    "\n",
    "The project is structured into four parts:\n",
    "\n",
    "#### Problem Selection and Analytical Solution:\n",
    "We begin by detailing the one-dimensional Schrödinger's Equation under a simple harmonic oscillator potential. This phase involves presenting the equation, outlining its physical significance, and deriving its analytical solution for a basic configuration. This solution will serve as a benchmark against which our computational results will be compared.\n",
    "\n",
    "#### Finite Difference Method Implementation:\n",
    "Next, we implement the Finite Difference Method (FDM) to numerically solve the Schrödinger's Equation. This traditional approach, well-established in numerical analysis, involves approximating derivatives by finite differences. We will meticulously develop the algorithm, execute it to solve our specific PDE, and then compare the outcomes with the analytical solution. This comparison aims to assess the accuracy and efficiency of the FDM in handling such quantum mechanical problems.\n",
    "\n",
    "#### Physics-Informed Neural Networks Implementation: \n",
    "Lastly, we explore the cutting-edge method of Physics-Informed Neural Networks (PINN). PINN represents a novel apprmachine where deep learning techniques are informed by the underlying ph of a systemysical laws, in our case, the Schrödinger'sGiven that neural networks can be designed to function as universal approximators, it is possible in principle to obtain the solution to our PDE with arbitrary accuaracy.  Equation. We wia neural network and train its output to find and fit the solution to Shrodinger's Equation that we seek. mplex PDEs.\n",
    "\n",
    "#### Comparison and Analysis:\n",
    "This segment is dedicated to comparison of the two computational methods—Finite Difference Method (FDM) and Physics-Informed Neural Networks (PINN)—against the analytical solution of the Schrödinger's Equation.\n",
    "\n",
    "Key elements of this segment include:\n",
    "**Accuracy Analysis:** We will assess the precision of both FDM and PINN by comparing their solutions to the analytical solution of the Schrödinger's Equation. This involves a quantitative analysis of the errors and discrepancies between the methods.\n",
    "\n",
    "**Efficiency Evaluation:** The computational efficiency of both methods will be evaluated in terms of processing time and resource utilization. This analysis will provide insight into the practicality of each method in different computational environments.\n",
    "\n",
    "**Methodological Insights:** We will delve into the strengths and limitations of each method. For instance, the robustness of FDM in handling well-defined computational grids versus the flexibility of PINN in dealing with more complex geometries and boundary conditions.\n",
    "\n",
    "**Theoretical Implications:** This phase will also explore the theoretical implications of our findings, particularly in the context of applying machine learning techniques to solve quantum mechanical problems.\n",
    "\n",
    "**Recommendations for Future Research:** Based on our comparative analysis, we will propose potential directions for future research, focusing on improving the methodologies, exploring other PDEs, and integrating other advanced computational techniques.\n",
    "\n",
    "By the end of this project, we aim to have practice and a deeper understanding of using these two methods to solve differential equations, and to draw meaningful comparisons between traditional numerical methods and modern machine learning approaches in the context of solving PDEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44baf9b5-52fc-40c7-84c8-b9bf60ed5e6f",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "\n",
    "# Problem Selection and Analytical Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41521a-92ea-45c0-939c-bc17e60dc648",
   "metadata": {},
   "source": [
    "##### Problem Setup\n",
    "- The infinite square well potential is defined as $ V(x) = 0 $ for $ 0 < x < L $ and $ V(x) = \\infty $ elsewhere.\n",
    "- The stationary states (eigenstates) of the system are given by the solutions to the time-independent Schrödinger equation.\n",
    "- In the infinite square well, the $n$-th stationary state is given by $\\psi_n(x)$ = $\\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{n\\pi x}{L}\\right) $, where $ n = 1, 2, 3, \\ldots $\n",
    "- The energy corresponding to each state is $ E_n = \\frac{n^2\\pi^2\\hbar^2}{2mL^2} $.\n",
    "\n",
    "##### Superposition of the First Two Stationary States\n",
    "- We choose to consider a state which is a superposition of the first two states, so $ \\psi(x) = A(\\psi_1(x) + \\psi_2(x)) $, where $ A $ is the normalization constant. This is analogous to a trapped particle with low energy, contributions to the overall state of the wavefunction from higher energies are negligible since these energies are unlikely. \n",
    "- The time-dependent wave function for each stationary state is $ \\psi_n(x,t) = \\psi_n(x) e^{-iE_nt/\\hbar} $.\n",
    "\n",
    "##### Derivation\n",
    "The expression for $\\psi(x, t)$ in the Jupyter notebook markdown cell format:\n",
    "\n",
    "\n",
    "$$\n",
    "\\psi(x, t) = A(\\psi_1(x, t) + \\psi_2(x, t))\n",
    "$$\n",
    "\n",
    "where $ \\psi_1(x, t) $ and $ \\psi_2(x, t) $ are the time-dependent wave functions for the first and second stationary states, respectively:\n",
    "\n",
    "$$\n",
    "\\psi_1(x, t) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{\\pi x}{L}\\right) e^{-iE_1t/\\hbar}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\psi_2(x, t) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{2\\pi x}{L}\\right) e^{-iE_2t/\\hbar}\n",
    "$$\n",
    "\n",
    "with the energies $ E_1 = \\frac{\\pi^2\\hbar^2}{2mL^2} $ and $ E_2 = \\frac{4\\pi^2\\hbar^2}{2mL^2} $. The full expression for $ \\psi(x, t) $ becomes:\n",
    "\n",
    "$$\n",
    "\\psi(x, t) = A\\left( \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{\\pi x}{L}\\right) e^{-i\\frac{\\pi^2\\hbar}{2mL^2}t} + \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{2\\pi x}{L}\\right) e^{-i\\frac{4\\pi^2\\hbar}{2mL^2}t} \\right)\n",
    "$$\n",
    "\n",
    "The normalization constant can be found easily, employing the orthogonality of the wave functions $ \\psi_1(x, t = 0) $ and $ \\psi_2(x, t = 0) $.\n",
    "\n",
    "$$\n",
    "\\psi(x, 0) = A(\\psi_1(x) + \\psi_2(x))\n",
    "$$\n",
    "\n",
    "The normalization condition requires that:\n",
    "\n",
    "$$\n",
    "\\int_0^L |\\psi(x, 0)|^2 dx = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "|\\psi(x, 0)|^2 = A^2[|\\psi_1(x)|^2 + |\\psi_2(x)|^2 + 2(\\psi_1(x) \\psi_2^*(x))]\n",
    "$$\n",
    "\n",
    "Since $ \\psi_1(x) $ and $ \\psi_2(x) $ are orthogonal, the cross term $ \\psi_1(x) \\psi_2^*(x) $ integrates to zero. \n",
    "\n",
    "$$\n",
    "\\int_0^L \\sin\\left(\\frac{n\\pi x}{L}\\right) \\sin\\left(\\frac{m\\pi x}{L}\\right) dx = 0 \\quad \\text{for } n \\neq m\n",
    "$$\n",
    "\n",
    "Therefore, we only need to consider the squares of the individual wave functions:\n",
    "\n",
    "$$\n",
    "|\\psi(x, 0)|^2 = A^2(|\\psi_1(x)|^2 + |\\psi_2(x)|^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int_0^L |\\psi_1(x)|^2 dx = \\int_0^L |\\psi_2(x)|^2 dx = 1\n",
    "$$\n",
    "\n",
    "The normalization condition becomes:\n",
    "\n",
    "$$\n",
    "A^2 \\left( \\int_0^L |\\psi_1(x)|^2 dx + \\int_0^L |\\psi_2(x)|^2 dx \\right) = 1\n",
    "$$\n",
    "\n",
    "Substituting the integrals:\n",
    "\n",
    "$$\n",
    "A^2(1 + 1) = 1 \\implies A^2 = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Therefore, the normalization constant $ A $ is:\n",
    "\n",
    "$$\n",
    "A = \\frac{1}{\\sqrt{2}}\n",
    "$$\n",
    "\n",
    "This ensures that the superposition state is properly normalized. The complete time-dependent wave function is then:\n",
    "\n",
    "$$\n",
    "\\psi(x, t) = \\frac{1}{\\sqrt{L}}\\left( \\sin\\left(\\frac{\\pi x}{L}\\right) e^{-i\\frac{\\pi^2\\hbar}{2mL^2}t} + \\sin\\left(\\frac{2\\pi x}{L}\\right) e^{-i\\frac{4\\pi^2\\hbar}{2mL^2}t} \\right)\n",
    "$$) for \\(n=0, 1, 2\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2f6a8-c3f4-42fd-89b3-1010a69170fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4374f128-e897-4f0a-afe3-d5b29662887c",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "\n",
    "# Finite Difference Method Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64546672-7ed0-419f-a966-b14df848216a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047cdf4c-b1e9-4264-a13b-fb1e0e06a563",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "\n",
    "# Physics-Informed Neural Networks Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5f067-7392-4667-9aa2-ea57f5a7625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Construction\n",
    "\n",
    "class MLP(nn.Module): # MultiLayerPerceptron\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # layers\n",
    "        self.il  = nn.Linear(2,20) # takes 2 inputs and maps to a layer with 50 nodes\n",
    "        self.hl1 = nn.Linear(20,20) # takes 50 inputs and maps to another layer with 50 nodes\n",
    "        self.hl2 = nn.Linear(20,20) # ... \n",
    "        self.hl3 = nn.Linear(20,20) # ...\n",
    "        self.ol  = nn.Linear(20,1) # takes 50 inputs and maps to the final layer with 1 node (this network outputs a scalar value)\n",
    "        # activation functions\n",
    "        self.tn  = nn.Tanh()\n",
    "        self.elu  = nn.ELU()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # setting up the forward model\n",
    "        u = torch.cat((x, y), 1)\n",
    "        u = self.il(u)\n",
    "        u = self.elu(u)\n",
    "        u = self.hl1(u)\n",
    "        u = self.elu(u)\n",
    "        u = self.hl2(u)\n",
    "        u = self.elu(u)\n",
    "        u = self.hl3(u)\n",
    "        u = self.elu(u)\n",
    "        u = self.ol(u)\n",
    "        return u\n",
    "\n",
    "NN_model = MLP()\n",
    "\n",
    "loss_evol = []\n",
    "optimizer = optim.Adam(NN_model.parameters(), lr=0.001) \n",
    "# Adam is a better optimizer for neural networks than simple gradient descent. It uses the same ideas as gradient descent but, this is more efficient.\n",
    "# optimizer = optim.SGD(NN_model.parameters(), lr=0.0001)\n",
    "\n",
    "# hyperparameters\n",
    "epocs = 1000\n",
    "n_batches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c98b59-2b5b-4909-9801-20d565defc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "for epoc in range(1, epocs+1):\n",
    "    n_points = X.shape[0]\n",
    "    idxs_shuffle = np.random.choice(n_points, size = n_points, replace = False)\n",
    "    for i in range(n_batches):\n",
    "        # batch of inputs\n",
    "        X_batch = X[idxs_shuffle[i::n_batches],:]\n",
    "        Y_batch = Y[idxs_shuffle[i::n_batches],:]\n",
    "        # batch of target outputs\n",
    "        F_batch = F[idxs_shuffle[i::n_batches],:]\n",
    "        \n",
    "        # batch of predicted outputs\n",
    "        output_batch = NN_model(X_batch, Y_batch)\n",
    "\n",
    "        # compute loss between target values and predicted values\n",
    "        loss = torch.mean(torch.square(output_batch-F_batch))\n",
    "        # keep track of loss function evolution to check if we are converging\n",
    "        loss_evol.append(loss.detach().numpy())\n",
    "        \n",
    "        # optimize -> 3 steps: \n",
    "        # zero the gradients calculated in the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # compute the new gradients using backward propagation\n",
    "        loss.backward()\n",
    "        # update the weights of the NN based on the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoc % 100 == 0:\n",
    "        print('epoc = ', epoc)\n",
    "        print('loss = ', float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c0b1d3-9edf-4385-8e46-509663d2ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final PINN Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82c492-4177-4597-9d48-70e1e4b2e81d",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "\n",
    "# Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcdbd8-c9a4-4fbb-a1d1-42a2960900c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
